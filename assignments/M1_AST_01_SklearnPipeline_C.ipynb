{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7f_QYB9HwEt"
      },
      "source": [
        "# Advanced Certification Programme in AI and MLOps\n",
        "## A programme by IISc and TalentSprint\n",
        "### Assignment 1: Pipeline Optimization with Scikit-Learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93OhtrPFI3Cv"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCt0SUxASryl"
      },
      "source": [
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "- appreciate the significance of a pipeline and its optimization\n",
        "- setup a machine learning pipeline\n",
        "- optimize the pipeline\n",
        "- know techniques to analyze the results of optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5uhFSfxJppc"
      },
      "source": [
        "## Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhtMbxlbSZOc"
      },
      "source": [
        "**ML Pipeline**\n",
        "\n",
        "A machine learning pipeline can be created by putting together a sequence of steps involved in training a machine learning model. It can be used to automate a machine learning workflow. The pipeline can involve pre-processing, feature selection, classification/regression, and post-processing steps. More complex applications may need to fit in other necessary steps within this pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eC1QBzs1OZO"
      },
      "source": [
        "**Problem Statement**\n",
        "\n",
        "Substances in traditional fire-extinguishing techniques may leave chemical waste, harm human health and cause social and economic damages. Therefore research on fire-extinguishing using renewable energy sources is important. The impact of sound waves on flame and combustion behavior of fuel is a common.\n",
        "\n",
        "Fire is a chemical reaction that breaks out with the combination of heat, fuel, and oxygen components. The heat, gas, and smoke resulting from this oxidation reaction may significantly harm to human and the environment. Early intervention to the fire facilitates to extinguish. However, **depending on the scale of the fire and the fuel type, fire-extinguishing agents may vary**. These substances in traditional fire-extinguishing techniques may leave chemical waste and harm human health. Additionally, it can also cause social and economic damages. In order to eliminate these impacts, researches on fire-extinguishing with renewable energy sources have been carried out. Currently, **the impact of sound waves on flame and combustion behavior of fuel is a common research topic**. The pressure changes in the air as a result of the sound waves lead to the occurrence of airflow. This airflow changes the behavior of the flame, fuel, and oxygen in the environment. The airflow created by the sound waves enables the fuel to spread over a wider surface. At this phase, the flame shows the tendency of spreading over a wide area together with the fuel. Fuel consumption also increases by the fuel particle oscillation due to the spread of flame and sound waves. While these stages are taking place, the air in the fire environment mixes and the amount of oxygen decreases as a result of the compression and expansion movements in the air. Through the combination of these three events, the flame can be extinguished. Necessary frequency ranges are available for the flame to be extinguished with the sound waves. Besides the frequency characteristic of sound waves, sound intensity level and the distance are also the factors having an impact on the ability to extinguish the flame.\n",
        "\n",
        "Utilizing the fire characteristics, studies have been carried out to estimate the parameters necessary for the detection and extinguishing of the fire. The **data have been obtained by examining the characteristics of the flames extinguished using sound waves**. Statistical analysis and classification algorithms using these data provide information on the behaviour of the flame.\n",
        "\n",
        "\n",
        "To know more about the experiment, click [here](https://ieeexplore.ieee.org/document/9452168)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk4BqqzecR1K"
      },
      "source": [
        "## Dataset Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuMzOs9LJ6zk"
      },
      "source": [
        "The **Acoustic Extinguisher Fire Dataset** was obtained as a result of the extinguishing tests of four different fuel flames with a sound wave extinguishing system. The sound wave fire-extinguishing system consists of 4 subwoofers with a total power of 4,000 Watt placed in the collimator cabinet. There are two amplifiers that enable the sound come to these subwoofers as boosted. Power supply that powers the system and filter circuit ensuring that the sound frequencies are properly transmitted to the system is located within the control unit. While computer is used as frequency source, anemometer was used to measure the airflow resulted from sound waves during the extinguishing phase of the flame, and a decibel meter to measure the sound intensity. An infrared thermometer was used to measure the temperature of the flame and the fuel can, and a camera is installed to detect the extinction time of the flame. A total of 17,442 tests were conducted with this experimental setup.\n",
        "\n",
        "The experiments are planned as follows:\n",
        "\n",
        "- Three different liquid fuels and LPG fuel were used to create the flame.\n",
        "- 5 different sizes of liquid fuel cans are used to achieve different size of flames.\n",
        "- Half and full gas adjustment is used for LPG fuel.\n",
        "- While carrying out each experiment, the fuel container, at 10 cm distance, was moved forward up to 190 cm by increasing the distance by 10 cm each time.\n",
        "- Along with the fuel container, anemometer and decibel meter were moved forward in the same dimensions.\n",
        "- Fire extinguishing experiments was conducted with 54 different frequency sound waves at each distance and flame size.\n",
        "Throughout the flame extinguishing experiments, the data obtained from each measurement device was recorded and a dataset was created.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO4pFGacmA0S"
      },
      "source": [
        "The dataset includes following features:\n",
        "\n",
        "- **SIZE:** *fuel container size representing the flame size* [7cm=1, 12cm=2, 14cm=3, 16cm=4, 20cm=5, Half throttle setting=6, Full throttle=7]\n",
        "- **FUEL:** *fuel type* [gasoline, thinner, kerosene, lpg]\n",
        "- **FREQUENCY:** *sound frequency* [1 - 75 Hz]\n",
        "- **DECIBEL:** *sound intensity* [72 - 113 dB]\n",
        "- **DISTANCE:** *bw fuel container and fire-extinguishing system* [10 - 190 cm]\n",
        "- **AIRFLOW:** *airflow resulted from sound waves* [0 - 17 m/s]\n",
        "- **STATUS:** *flame extinction* [0 indicates the non-extinction state, 1 indicates the extinction state] ***(dependent/target variable)***\n",
        "\n",
        "Accordingly, 6 input features and 1 output feature will be used in models.\n",
        "The status property (flame extinction or non-extinction states) can be predicted by using six features in the dataset. Status and fuel features are categorical, while other features are numerical. 8,759 of the 17,442 test results are the non-extinguishing state of the flame. 8,683 of them are the extinction state of the flame. According to these numbers, it can be said that the class distribution of the dataset is almost equal.\n",
        "\n",
        "To know more about the dataset, click [here](https://www.kaggle.com/datasets/muratkokludataset/acoustic-extinguisher-fire-dataset)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u06XHm-kzeT5"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YzfoPvJDiTX"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjoZJWGErxGf"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WBPPuGmBlDIN"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M1_AST_01_SklearnPipeline_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/AIandMLOps/Datasets/Acoustic_Extinguisher_Fire_Dataset.xlsx\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://aimlops-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69RDKc0Rq6Le"
      },
      "source": [
        "### Import Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88_C4BgAFCH2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt                                 # For plotting data\n",
        "import seaborn as sns                                           # For plotting data\n",
        "from sklearn.model_selection import train_test_split            # For train/test splits\n",
        "from sklearn.tree import DecisionTreeClassifier                 # Classifier model\n",
        "from sklearn.feature_selection import VarianceThreshold         # Feature selector\n",
        "from sklearn.pipeline import Pipeline                           # For setting up pipeline\n",
        "\n",
        "# Various pre-processing steps\n",
        "from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler, PowerTransformer, MaxAbsScaler, LabelEncoder, OrdinalEncoder\n",
        "from sklearn.model_selection import GridSearchCV                # For optimization\n",
        "\n",
        "# To supress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49YbZU_erJn4"
      },
      "source": [
        "### Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69at-tNsTDcC"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel('Acoustic_Extinguisher_Fire_Dataset.xlsx', sheet_name='A_E_Fire_Dataset')\n",
        "\n",
        "# Shape of dataframe\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H53GhbkwrQMw"
      },
      "source": [
        "### Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oefdmDE_UiCc"
      },
      "outputs": [],
      "source": [
        "# Show first few rows of dataframe\n",
        "df.head(8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVIuNmCPsDNn"
      },
      "source": [
        "From above it can be seen that:\n",
        "\n",
        "- There are 6 independent variables\n",
        "- `FUEL` is a categorical feature\n",
        "- Every other feature is numerical\n",
        "- `STATUS` is the dependent variable\n",
        "- It is a binary classification problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ot-_opHsraf"
      },
      "source": [
        "### Segregating the dataframe into independent and dependent features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xcV5GOXcDT7"
      },
      "outputs": [],
      "source": [
        "# The data matrix X\n",
        "X = df.iloc[:, :-1]\n",
        "\n",
        "# The labels\n",
        "y = (df.iloc[:,-1:])\n",
        "\n",
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHGQcdQAdth6"
      },
      "outputs": [],
      "source": [
        "# Independent features\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifhK8xQhtC2N"
      },
      "source": [
        "### Exploring the unique categories in categorical feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4r1aHQtfibA"
      },
      "outputs": [],
      "source": [
        "# Unique values in FUEL column\n",
        "X['FUEL'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tPMhs0YvnAP"
      },
      "outputs": [],
      "source": [
        "# Plot unique values in FUEL column\n",
        "uniques = X['FUEL'].value_counts()\n",
        "sns.barplot(x = uniques.index,\n",
        "            y = uniques.values,\n",
        "            hue=uniques.index            # color\n",
        "            )\n",
        "plt.xlabel(\"FUEL type\")\n",
        "plt.ylabel(\"Values\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMhEIvL-kCSA"
      },
      "source": [
        "### Encoding the Categorical Feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLEf_IV8kAEW"
      },
      "outputs": [],
      "source": [
        "# Ordinal encode input variable\n",
        "ordinal = OrdinalEncoder()\n",
        "X['FUEL'] = ordinal.fit_transform(X[['FUEL']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDEmTnxG0Dvh"
      },
      "outputs": [],
      "source": [
        "X.tail(110)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REdtfeKftWr1"
      },
      "source": [
        "After ordinal encoding, all the features are numerical in nature now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cUIcML23n-L"
      },
      "outputs": [],
      "source": [
        "# Prediction features\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qdzBidudvYk"
      },
      "outputs": [],
      "source": [
        "# Target feature: Extinction Status\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxH87eA-tgZT"
      },
      "source": [
        "### Split the data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4y-f5vXOdxtN"
      },
      "outputs": [],
      "source": [
        "# Split the data into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,                  # predictors\n",
        "                                                    y,                  # labels\n",
        "                                                    test_size=1/3,      # test set size\n",
        "                                                    random_state=0)     # set random number generator seed for reproducibility\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81xxistBUov5"
      },
      "source": [
        "### A Classifier Without a Pipeline and Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxxfOS4W_gJZ"
      },
      "source": [
        "First, let's just check how the DecisionTree performs on the training and test sets. This would give us a baseline for performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate DecisionTree classifier and fit on train set\n",
        "dt_model = DecisionTreeClassifier(max_depth = 4,              # The maximum depth of the tree\n",
        "                                  max_features = 3,             # The number of features to consider when looking for the best split\n",
        "                                  min_samples_leaf = 4              # The minimum number of samples required to be at a leaf node\n",
        "                                  )\n",
        "\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "# Performance on train and test sets\n",
        "print('Training set score: ' + str(dt_model.score(X_train,y_train)))\n",
        "print('Test set score: ' + str(dt_model.score(X_test,y_test)))"
      ],
      "metadata": {
        "id": "8ZCOh1nn1ocA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ0JCvWsU7-e"
      },
      "source": [
        "### Setting Up a Machine Learning Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLeFDVre_SZS"
      },
      "source": [
        "- **Scaler:** For pre-processing data, i.e., transform the data to zero mean and unit variance using the `StandardScaler()`.\n",
        "    - Scaling the features is not necessary for Tree based models like decision tree, xgboost, etc. Here, scaling is included inside pipeline for demonstration purpose.\n",
        "\n",
        "- **Feature selector:** Use `VarianceThreshold()` for discarding features whose variance is less than a certain defined threshold.\n",
        "\n",
        "- **Classifier:** `DecisionTreeClassifier()`, which implements a decision tree classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoXhOD7HUv8m"
      },
      "outputs": [],
      "source": [
        "# Setup pipeline\n",
        "pipe = Pipeline([('scaler', StandardScaler()),\n",
        "                 ('selector', VarianceThreshold()),\n",
        "                 ('classifier', DecisionTreeClassifier(max_depth = 4, max_features = 3, min_samples_leaf = 4))\n",
        "                 ])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit pipeline on train set\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "# Performance on train and test sets\n",
        "print('Training set score: ' + str(pipe.score(X_train,y_train)))\n",
        "print('Test set score: ' + str(pipe.score(X_test,y_test)))"
      ],
      "metadata": {
        "id": "OuFKGI5l3wVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fowNWKfFVTx_"
      },
      "source": [
        "### Optimizing and Tuning the Pipeline with GridSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLDvhYpwnNFw"
      },
      "source": [
        "In the code below, weâ€™ll show the following:\n",
        "\n",
        "- We can search for the best scalers. Instead of just the `StandardScaler()`, we can try `MinMaxScaler()`, `Normalizer()`, and `MaxAbsScaler()`.\n",
        "\n",
        "- We can search for the best variance threshold to use in the selector, i.e., `VarianceThreshold()`.\n",
        "\n",
        "- We can search for the best value of max_depth for the `DecisionTreeClassifier()`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {'scaler': [StandardScaler(), MinMaxScaler(), Normalizer(), MaxAbsScaler()],\n",
        "              'selector__threshold': [0, 0.001, 0.01],\n",
        "\t\t\t\t\t\t\t'classifier__max_depth': [3, 4, 5, 6, 7],           # The maximum depth of the tree\n",
        "\t\t\t\t\t\t\t'classifier__max_features': [3, 4, 5],               # The number of features to consider when looking for the best split\n",
        "\t\t\t\t\t\t\t'classifier__min_samples_leaf': [1, 2, 3, 4]                # The minimum number of samples required to be at a leaf node\n",
        "\t\t\t  }"
      ],
      "metadata": {
        "id": "xxy56w0E4kii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4*3*5*3*4"
      ],
      "metadata": {
        "id": "3djnpgao6WD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mdtg8rcU_nEa"
      },
      "source": [
        "Here, we have passed a list of parameters that the GridsearchCV algorithm will use to come at an optimum solution. It will go through every combination of this parameters to get an optimal solution. So, total iterations here will be 4 $\\times$ 3 $\\times$ 5 $\\times$ 3 $\\times$ 4 = 720.\n",
        "\n",
        "`max_depth`, `max_features` and `min_samples_leaf` are the parameter for `DecisionTreeClassifier()`.\n",
        "\n",
        "To know more about `DecisionTreeClassifier()`, refer [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate GridSearchCV\n",
        "grid = GridSearchCV(estimator = pipe,                    # a scikit-learn model or pipeline\n",
        "                    param_grid = parameters,             # Dictionary with parameters names as keys and lists of parameter settings to try as values\n",
        "                    cv=2                             # Determines the cross-validation splitting strategy; to specify the number of folds in a (Stratified)KFold\n",
        "                    )\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Performance on train and test sets\n",
        "print('Training set score: ' + str(grid.score(X_train, y_train)))\n",
        "print('Test set score: ' + str(grid.score(X_test, y_test)))"
      ],
      "metadata": {
        "id": "Jswi6u5y7jBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCnKM224AIMm"
      },
      "source": [
        "To know more about GridSearchCV, refer [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zNZEXAFo3a2"
      },
      "source": [
        "### Analyzing the Results\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the best set of parameters\n",
        "best_params = grid.best_params_\n",
        "print(\"Best set of parameters:\")\n",
        "print(best_params)\n",
        "\n",
        "# Stores the optimum model in best_pipe\n",
        "best_pipe = grid.best_estimator_\n",
        "print(\"\\nOptimum pipeline:\")\n",
        "print(best_pipe)"
      ],
      "metadata": {
        "id": "vhXD4Uyh8D9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbarYPUJif0z"
      },
      "source": [
        "Another useful technique for analyzing the results is to construct a DataFrame from the `grid.cv_results_` attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJiM_ZhEo9pt"
      },
      "outputs": [],
      "source": [
        "# Create a dataframe\n",
        "result_df = pd.DataFrame.from_dict(grid.cv_results_, orient='columns')\n",
        "print(result_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYYRy9QQc3vK"
      },
      "outputs": [],
      "source": [
        "result_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiJHUyPTczoj"
      },
      "outputs": [],
      "source": [
        "result_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGRrwxgACPJW"
      },
      "source": [
        "This DataFrame is very valuable as it shows us the scores for different parameters. The column with the `mean_test_score` is the average of the scores on the test set for all the folds during cross-validation. The DataFrame may be too big to visualize manually, hence, it is always a good idea to plot the results.\n",
        "\n",
        "Let's see how `max_depth` affect the performance for different `scalers`, and for different values of `max_features`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5YjM-2Lpw8y"
      },
      "outputs": [],
      "source": [
        "sns.relplot(data = result_df,\n",
        "            kind = 'line',\n",
        "\t\t\tx = 'param_classifier__max_depth',\n",
        "\t\t\ty = 'mean_test_score',\n",
        "\t\t\thue = 'param_scaler',\n",
        "\t\t\tcol = 'param_classifier__max_features')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UFZTncnDBDb"
      },
      "source": [
        "From the above plots, it can be seen that:\n",
        "\n",
        "- For all `max_features = 3, 4, 5`, worst performing scaler method is `Normalizer()`\n",
        "- For all `max_features = 3, 4, 5`, best performing scaler is `MaxAbsScaler()`\n",
        "- Since the `mean_test_score` is still increasing we could check for few more sets of parameter values.\n",
        "\n",
        "Let's see how `max_depth` affect the performance for different `scalers`, and for different values of `min_samples_leaf`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VB9NpUkRqi58"
      },
      "outputs": [],
      "source": [
        "sns.relplot(data = result_df,\n",
        "            kind = 'line',\n",
        "\t\t\tx = 'param_classifier__max_depth',\n",
        "\t\t\ty = 'mean_test_score',\n",
        "\t\t\thue = 'param_scaler',\n",
        "\t\t\tcol = 'param_classifier__min_samples_leaf')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAZD73LSDw3q"
      },
      "source": [
        "- For all the `min_sampls_leaf` parameters, that is 1, 2, 3, 4, worst performing scaler method is `Normalizer()`.\n",
        "\n",
        "- Since the `mean_test_score` is still increasing we might want to check for few more sets of parameter values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1fpVuU87D0i"
      },
      "source": [
        "`MaxAbsScaler()` performs well because it scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be $1.0$. It does not shift or center the data, and thus does not destroy any sparsity.\n",
        "\n",
        "To know more about `MaxAbsScaler()`, refer [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html#sklearn.preprocessing.MaxAbsScaler)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "outputs": [],
      "source": [
        "#@title Select the False statement regarding Scikit-learn Pipeline: { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"\" #@param [\"\", \"With pipeline, all the preprocessing steps needs to be done separately for both training and testing set\", \"Without pipeline, the parameters used for preprocessing of training set needs to be stored for preprocessing testing set\", \"With pipeline, doing the same preprocessing steps twice can be avoided\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "outputs": [],
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "outputs": [],
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "outputs": [],
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "outputs": [],
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "outputs": [],
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}